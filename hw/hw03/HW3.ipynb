{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd91e7f3-3db3-465f-8437-5c604d3f2494",
   "metadata": {},
   "source": [
    "## HW 3: Evaluating language models \n",
    "### COSC 426: Fall 2025, Colgate University\n",
    "\n",
    "Use this notebook to run your experiments for Part 1, load and display from your experiment from Part 2, and answer the questions in all parts. Feel free to add as many code and markdown chunks as you would like in each of the sub-sections. \n",
    "\n",
    "**If you use any external resources (e.g., code snippets, reference articles), please cite them in comments or text!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96443a87-57c4-4c18-a825-9e9aa8cd0f32",
   "metadata": {},
   "source": [
    "## Part 1: Train and evaluate bigram and trigram models \n",
    "\n",
    "In this section, include the experiments to train and evaluate bigram and trigram models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18832e8d-7d6f-4f41-b0f4-66774c1b693e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mHW3\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/NLP_Class/cosc426/hw/hw03/HW3.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_ngrams\u001b[39m(text:\u001b[38;5;28mlist\u001b[39m, n):\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    Params:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m        text: tokenized text in a list\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m        list of all ngrams\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import HW3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c259f82b-a926-43f3-9692-767130347103",
   "metadata": {},
   "source": [
    "## Part 2: Train and evaluate `distilgpt2` model\n",
    "\n",
    "In this part, load in the results from your trained `distilgpt2` model from NLPScholar, and evaluate the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3e5180-baba-4983-9614-8699a17b2deb",
   "metadata": {},
   "source": [
    "## Part 3: Reflect on the role of tokenizer\n",
    "In this part, answer the question in `HW3.md` in markdown chunks. If you used external sources to find and make sense of this, please cite them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7524849f-ba17-450e-88d6-2927644d0130",
   "metadata": {},
   "source": [
    "## Part 4 (Optional): Explore the effect of tokenizer and vocab on ngram model performance\n",
    "\n",
    "In this part, include the results from your experiments (if you choose to attempt this)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
