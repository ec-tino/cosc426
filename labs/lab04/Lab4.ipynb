{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f397c0dc-1fed-4f5b-89a8-e0af21e06c0a",
   "metadata": {},
   "source": [
    "# Lab 3: Grammar writing\n",
    "### COSC 426: Fall 2025, Colgate University\n",
    "\n",
    "Use this notebook to answer the questions in `Lab4.md`. Make sure to include in this notebook all the tests you run to ensure that at each stage your n-gram model implementation is correct. Please use **markdown** chunks to answer any non-code questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbda0e4a-6302-4d4e-8c55-cc75a8648a0c",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "Answer questions for each of the subparts here. Add as many markdown chunks as needed under each sub-part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef565601-0982-4932-a12d-290e1dd28b60",
   "metadata": {},
   "source": [
    "### Part 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acc186d",
   "metadata": {},
   "source": [
    "C(the, panda) = 1\n",
    "C(the) = 3\n",
    "P(panda | the) = 1/3\n",
    "\n",
    "C(a, sandwich) = 1\n",
    "C(a) = 2\n",
    "P(sandwich | a) = 1/2\n",
    "\n",
    "C(in, the) = 0\n",
    "C(in) = 2\n",
    "P(the | in) = 0/2\n",
    "\n",
    "C(panda, eats) = 1\n",
    "C(panda) = 3\n",
    "P(eat | panda) = 1/3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ea82ca-2c1a-437a-824e-4620c4449a5b",
   "metadata": {},
   "source": [
    "### Part 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999daa2c",
   "metadata": {},
   "source": [
    "C([BOS], the) = 2\n",
    "C([BOS]) = 3\n",
    "P(the | [BOS]) = 2/3\n",
    "\n",
    "C([UNK], panda) = 1\n",
    "C([UNK]) = 5\n",
    "P(panda|[UNK]) = 1/5\n",
    "\n",
    "C(likes, [UNK]) = 1\n",
    "C(likes) = 2\n",
    "P([UNk] | likes) = 1/2\n",
    "\n",
    "C([UNK], peacefully) = 0\n",
    "C([UNK]) = 5\n",
    "P(peacefully | [UNK]) = 0\n",
    "\n",
    "C([UNK], [UNK]) = 2\n",
    "C([UNK]) = 5\n",
    "P([UNK] | [UNK]) = 2/5\n",
    "\n",
    "C(panda, [EOS]) = 1\n",
    "C(panda) = 3\n",
    "P([EOS]|panda) = 1/3\n",
    "\n",
    "C(likes, sandwich) = 0\n",
    "C(likes) = 3\n",
    "P(sandwich | likes) = 0\n",
    "\n",
    "It assigns high probality for words that are more likely to begin a sentence. It also helps to differentiate between sentences; to know when a sentence ends and when it begins.\n",
    "\n",
    "Words that are not present in the vocan with out the tag will have a prob of 0, so with the UNK tag, it helps you quantify how many words are not present in the vocab. This helps to assign somewhat better probabilities to contexts where we have unknown words.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6fd57d-96f3-4c25-ab34-eabcbb64bc2a",
   "metadata": {},
   "source": [
    "### Part 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dad7c1",
   "metadata": {},
   "source": [
    "Add 1 smoothing add one to both numerator and the count vacab size.\n",
    "P(panda | the) = 1 + 1/3 + 10 = 2/13\n",
    "\n",
    "P(sandwich | a) = 1+1 / 2 + 10 = 1/6\n",
    "\n",
    "P(the | in) = 0 + 1 / 2 + 10 = 1/12\n",
    "\n",
    "P(eat | panda) = 1+1 / 3+10 = 2/13\n",
    "\n",
    "P(the | [BOS]) = 2 + 1 / 3 + 10 = 3/13\n",
    "\n",
    "P([UNK] | [UNK]) = 2+1 / 5+10 = 1/5\n",
    "\n",
    "P([EOS]|panda) = 1+1 / 3+10 = 2/13\n",
    "\n",
    "1. There might be words sequence that appear in the vocabulary but not in the corpus; if there is no smoothing their count/probability will be zero. By smoothing, we can account for such combination of words and assign some probability to their occurence. If we are ever evaluating a model, we want the model to know that the those combination of words are possible, and should not assign zero  probability to those. And we achieve this by smoothing. \n",
    "\n",
    "2. It reduces the probability of the bigrams. It distributes some of the known probailities to some unknown probabilities. \n",
    "\n",
    "3. add - 0.1; this is because it reduces the unsmoothed bigram probability estimates the least. Compared with the unsmoothed probabilities, the add-0.1 probabilities are closest to the initial probabailities without smoothing.  \n",
    "\n",
    "4. Smaller values of k will be better. This will redistribute very small portion of our probability mask to unknown texts. This is preferred because our training text is very similar to our text text. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00491d7c-b24b-424a-a314-72add06b1283",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Use this part to test your code. Feel free to add any additional code chunks. Note, once you make changes to `Lab4.py` you might have to restart the kernel for the changes to be reflected here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41733e9b-9e0e-42ef-a958-39dd5600f660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/ernestclottey/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.6666666666666666\n",
      "0.0\n",
      "0.0\n",
      "Trying:\n",
      "    TestBigramFreqs(getBigramFreqs(preprocess('data/test.txt', mark_ends=True), getVocab('data/glove_vocab.txt')))\n",
      "Expecting:\n",
      "    {1: 70, 2: 3}\n",
      "ok\n",
      "Trying:\n",
      "    TestBigramFreqs(getBigramFreqs(preprocess('data/test.txt', mark_ends=True), getVocab('data/glove_vocab.txt')), print_non1=True)\n",
      "Expecting:\n",
      "    {2: [('kitten', 'had'), ('.', '[EOS]'), ('for', 'the')]}\n",
      "ok\n",
      "Trying:\n",
      "    len(getVocab('data/glove_vocab.txt'))\n",
      "Expecting:\n",
      "    400003\n",
      "ok\n",
      "Trying:\n",
      "    preprocess('data/test.txt', mark_ends=True)\n",
      "Expecting:\n",
      "    [['[BOS]', 'one', 'thing', 'was', 'certain', ',', 'that', 'the', '_white_', 'kitten', 'had', 'had', 'nothing', 'to', 'do', 'with', 'it', ':', '—it', 'was', 'the', 'black', 'kitten', '’', 's', 'fault', 'entirely', '.', '[EOS]'], ['[BOS]', 'for', 'the', 'white', 'kitten', 'had', 'been', 'having', 'its', 'face', 'washed', 'by', 'the', 'old', 'cat', 'for', 'the', 'last', 'quarter', 'of', 'an', 'hour', '(', 'and', 'bearing', 'it', 'pretty', 'well', ',', 'considering', ')', ';', 'so', 'you', 'see', 'that', 'it', '_couldn', '’', 't_', 'have', 'had', 'any', 'hand', 'in', 'the', 'mischief', '.', '[EOS]']]\n",
      "ok\n",
      "Trying:\n",
      "    preprocess('data/test.txt', mark_ends=False)\n",
      "Expecting:\n",
      "    [['one', 'thing', 'was', 'certain', ',', 'that', 'the', '_white_', 'kitten', 'had', 'had', 'nothing', 'to', 'do', 'with', 'it', ':', '—it', 'was', 'the', 'black', 'kitten', '’', 's', 'fault', 'entirely', '.'], ['for', 'the', 'white', 'kitten', 'had', 'been', 'having', 'its', 'face', 'washed', 'by', 'the', 'old', 'cat', 'for', 'the', 'last', 'quarter', 'of', 'an', 'hour', '(', 'and', 'bearing', 'it', 'pretty', 'well', ',', 'considering', ')', ';', 'so', 'you', 'see', 'that', 'it', '_couldn', '’', 't_', 'have', 'had', 'any', 'hand', 'in', 'the', 'mischief', '.']]\n",
      "ok\n",
      "7 items had no tests:\n",
      "    Lab4\n",
      "    Lab4.TestBigramFreqs\n",
      "    Lab4.corpus_size\n",
      "    Lab4.evaluate_model\n",
      "    Lab4.getBigramProb\n",
      "    Lab4.get_word_freq_dict\n",
      "    Lab4.main\n",
      "\u001b[32m3 items passed all tests:\u001b[0m\n",
      " \u001b[32m  2 tests in Lab4.getBigramFreqs\u001b[0m\n",
      " \u001b[32m  1 test in Lab4.getVocab\u001b[0m\n",
      " \u001b[32m  2 tests in Lab4.preprocess\u001b[0m\n",
      "5 tests in 10 items.\n",
      "\u001b[32m5 passed\u001b[0m.\n",
      "\u001b[1;32mTest passed.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TestResults(failed=0, attempted=5)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## DO NOT CHANGE THIS CHUNK\n",
    "import Lab4\n",
    "import doctest\n",
    "\n",
    "doctest.testmod(Lab4, verbose=True) ## runs the doctest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0618d5ee-276a-4eeb-9d1b-1afaeb6668d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correct probs to test your implementation of getBigramProb()\n",
    "correct_probs_mle = {\n",
    "        ('one', 'thing'): 1.0,\n",
    "        ('kitten', 'had'): 0.6666666666666666,\n",
    "        ('cat', 'had'): 0.0,\n",
    "        ('had', 'had'): 0.25,\n",
    "        ('on', 'the'): 0.0,\n",
    "        ('held', 'a'): 0.0,\n",
    "        ('zzzzzzz', 'the'): 0.0\n",
    "    }\n",
    "\n",
    "correct_probs_add1 = {\n",
    "        ('one', 'thing'): 4.999950000499995e-06,\n",
    "        ('kitten', 'had'): 7.499887501687475e-06,\n",
    "        ('cat', 'had'): 2.4999750002499977e-06,\n",
    "        ('had', 'had'): 4.999912501531223e-06,\n",
    "        ('on', 'the'): 2.499981250140624e-06,\n",
    "        ('held', 'a'): 2.499981250140624e-06,\n",
    "        ('zzzzzzz', 'the'): 2.4999562507656116e-06\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fcbc77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Lab4.getVocab('data/glove_vocab.txt')\n",
    "\n",
    "#train on through_the_looking_glass.txt\n",
    "sentences = Lab4.preprocess('data/test.txt', mark_ends=True)\n",
    "\n",
    "#this is dictionary for through_the_looking_glass file\n",
    "bigram_freq = Lab4.getBigramFreqs(sentences, vocab)\n",
    "word_freq = Lab4.get_word_freq_dict(sentences, vocab) #of my training data/text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bde635d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#code to test getprob. \n",
    "for key in correct_probs_mle:\n",
    "    print(Lab4.getBigramProb(key, \"MLE\", bigram_dict=bigram_freq, vocabs = vocab, dict_word_freq=word_freq) == correct_probs_mle[key])\n",
    "    \n",
    "for key in correct_probs_add1:\n",
    "    # print(key)\n",
    "    # print(correct_probs_add1[key])\n",
    "    # print(Lab4.getBigramProb(key, \"add-1\", bigram_dict=bigram_freq, vocabs = vocab, dict_word_freq=word_freq))\n",
    "   print(Lab4.getBigramProb(key, \"add-1\", bigram_dict=bigram_freq, vocabs = vocab, dict_word_freq=word_freq) == correct_probs_add1[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82ac08b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('one', 'thing')\n",
      "4.999950000499995e-06\n",
      "4.999950000499995e-06\n",
      "('kitten', 'had')\n",
      "7.499887501687475e-06\n",
      "7.499887501687475e-06\n",
      "('cat', 'had')\n",
      "2.4999750002499977e-06\n",
      "2.4999750002499977e-06\n",
      "('had', 'had')\n",
      "4.999912501531223e-06\n",
      "4.999912501531223e-06\n",
      "('on', 'the')\n",
      "2.499981250140624e-06\n",
      "2.4999562507656116e-06\n",
      "('held', 'a')\n",
      "2.499981250140624e-06\n",
      "2.4999562507656116e-06\n",
      "('zzzzzzz', 'the')\n",
      "2.4999562507656116e-06\n",
      "2.4999562507656116e-06\n"
     ]
    }
   ],
   "source": [
    "for key in correct_probs_add1:\n",
    "    print(key)\n",
    "    print(correct_probs_add1[key])\n",
    "    print(Lab4.getBigramProb(key, \"add-1\", bigram_dict=bigram_freq, vocabs = vocab, dict_word_freq=word_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5680bf1",
   "metadata": {},
   "source": [
    "The bigrams ('held', 'a') and ('on', 'the') not pass the tests i run on getBrigramProb. The values are teh same up to the fourth decimal place, and I am not particularly sure about what is causing that problem.\n",
    "\n",
    "For ('on', 'the')\n",
    "expected: 2.499981250140624e-06\n",
    "got: 2.4999562507656116e-06\n",
    "\n",
    "For ('held', 'a')\n",
    "expected: 2.499981250140624e-06\n",
    "got: 2.4999562507656116e-06"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ce58a-2b2d-4e3a-80fa-d1b54377585b",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "Use this part to answer questions in Part 3. Add as many code and markdown chunks as is helpful. **Remember, once you make changes to your `Lab4.py`, you need to restart the kernel and reimport the file**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b1cd0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37436.64615981037\n"
     ]
    }
   ],
   "source": [
    "#vocab is preloaded above\n",
    "#vocab = Lab4.getVocab('data/glove_vocab.txt')\n",
    "\n",
    "#train on through_the_looking_glass.txt\n",
    "sentences = Lab4.preprocess('data/through_the_looking_glass.txt', mark_ends=False)\n",
    "\n",
    "#this is dictionary for through_the_looking_glass file\n",
    "bigram_freq = Lab4.getBigramFreqs(sentences, vocab)\n",
    "word_freq = Lab4.get_word_freq_dict(sentences, vocab) #of my training data/text\n",
    "\n",
    "N = Lab4.corpus_size(sentences)\n",
    "\n",
    "#evaluating the model on through_the_looking_glass.txt\n",
    "print(Lab4.evaluate_model(N, sentences, \"add-1\", vocab, bigram_freq, word_freq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cd49a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55264.87572188816\n"
     ]
    }
   ],
   "source": [
    "# evaluate on sherlock, change sentences to the corpus from sherlock text. \n",
    "# But maintain the bigram_freq dictionary from through_the_looking_glass\n",
    "sentences_alice = Lab4.preprocess('data/alice_in_wonderland.txt', mark_ends=False)\n",
    "N_alice = Lab4.corpus_size(sentences_alice)\n",
    "#word_freq = Lab4.get_word_freq_dict(sentences_alice)\n",
    "\n",
    "#evaluating the model on alice_in_wonderland.txt\n",
    "print(Lab4.evaluate_model(N_alice, sentences_alice, \"add-1\", vocab, bigram_freq, word_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49458e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87140.57425787832\n"
     ]
    }
   ],
   "source": [
    "sentences_sherlock = Lab4.preprocess('data/sherlock_holmes.txt', mark_ends=False)\n",
    "N_sherlock = Lab4.corpus_size(sentences_sherlock)\n",
    "\n",
    "#evaluating the model on sherlock_holmes.txt\n",
    "print(Lab4.evaluate_model(N_sherlock, sentences_sherlock, \"add-1\", vocab, bigram_freq, word_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04bf20c",
   "metadata": {},
   "source": [
    "Perplexity, which measures a model's surprise with a given text, varies depending on the content. \"Through the Looking-Glass\" resulted in the lowest perplexity after evaluation, which is expected since the model was trained on that specific text and is thus less surprised by it. The \"sherlock.txt\" file, being longer, had a higher perplexity. This is likely because the model encountered a greater number of new words, leading to increased surprisal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f12f1a-e4ba-469a-9e47-22796546569c",
   "metadata": {},
   "source": [
    "## Part 4 (optional)\n",
    "Use this part to answer questions in Part 3. Add as many code and markdown chunks as is helpful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
