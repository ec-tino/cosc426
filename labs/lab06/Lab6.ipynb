{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a26dada8-c3f6-4c7c-8a74-e34854677dc2",
   "metadata": {},
   "source": [
    "# Lab 6: (Sub)Word embeddings\n",
    "### COSC 426: Fall 2025, Colgate University\n",
    "\n",
    "Use this notebook to answer the questions in `Lab6.md`. Make sure to include in this notebook all the tests and experiments you run. Make sure to also cite any external resources you use. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e780bc-f9e2-450f-8d9a-e921ffead863",
   "metadata": {},
   "source": [
    "## Part 1: Computing similarity between `GLoVe` embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82844cdc-aafa-4ce4-9343-267921c5bc5c",
   "metadata": {},
   "source": [
    "### Part 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f52df1-7548-4ac0-ae62-58c38847b7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open('glove_dolma_300_10k.pkl', 'rb') as f:\n",
    "    glove_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1ea518-bc91-49ff-8f94-6401a09fd8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(glove_embeddings.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539ab3b9-ebfc-4fac-8044-6a350c36a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['hello', 'cat', 'chomsky', 'supercalifragilisticexpialidocious']\n",
    "\n",
    "for word in words:\n",
    "    if word in glove_embeddings:\n",
    "        vec = glove_embeddings[word]\n",
    "        print(f'\"{word}\": Dimensions', vec.shape, \"Mean:\", np.mean(vec))\n",
    "    else:\n",
    "        print(f'\"{word}\": Missing embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a53157f-e2db-48f6-af6c-0effe00eaa59",
   "metadata": {},
   "source": [
    "### Part 1.2\n",
    "\n",
    "Implement and test the following functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c846dc-f850-4e49-b7dc-efa54f89f562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector_glove(word:str,embeddings:dict):\n",
    "    \"\"\"\n",
    "    Return embedding of word if it exists, if not the mean embedding of all words\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1d533f-efd2-4a89-bde2-026040dbd58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tests\n",
    "print(str(get_word_vector_glove('hello',glove_embeddings).shape) == '(300,)')\n",
    "print(str(np.mean(get_word_vector_glove('hello',glove_embeddings))) == '-5.767882e-05')\n",
    "print(str(get_word_vector_glove('hello',glove_embeddings)[0]) == '0.200562')\n",
    "\n",
    "print(str(get_word_vector_glove('supercalifragilisticexpialidocious',glove_embeddings).shape) == '(300,)')\n",
    "print(str(np.mean(get_word_vector_glove('supercalifragilisticexpialidocious',glove_embeddings))) == '-0.0065580728')\n",
    "print(str(get_word_vector_glove('supercalifragilisticexpialidocious',glove_embeddings)[0]) == '-0.08470377')\n",
    "\n",
    "print(str(get_word_vector_glove('chomsky',glove_embeddings).shape) == '(300,)')\n",
    "print(str(np.mean(get_word_vector_glove('chomsky',glove_embeddings))) == '-0.0065580728')\n",
    "print(str(get_word_vector_glove('chomsky',glove_embeddings)[0]) == '-0.08470377')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084e1ee7-f71e-4e9d-ad07-64d8f17680f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1:np.array, vec2:np.array):\n",
    "    \"\"\"\n",
    "    Returns cosine similarity between two vectors\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04ee40a-8385-42cd-999e-3ae6e4a9b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(cosine_similarity(np.array([1,2,3]), np.array([1,2, -3]))) == '-0.2857142857142857')\n",
    "print(str(cosine_similarity(np.array([1,2,3]), np.array([1,-2, 3]))) == '0.42857142857142855')\n",
    "print(str(cosine_similarity(np.array([1,2,3]), np.array([1,2, 3]))) == '1.0')\n",
    "\n",
    "sims = {'hello hello': '1.0000001',\n",
    "        'hello hey': '0.93034637',\n",
    "        'hello hi': '0.9133941',\n",
    "        'hello supercalifragilisticexpialidociou': '0.73929405',\n",
    "        'hello chomsky': '0.73929405',\n",
    "        'hello cat': '0.6564517'\n",
    "       }\n",
    "\n",
    "for key,val in sims.items():\n",
    "    word1, word2 = key.split()\n",
    "    res = str(cosine_similarity(get_word_vector_glove(word1,glove_embeddings),\n",
    "                  get_word_vector_glove(word2,glove_embeddings)))\n",
    "\n",
    "    print(res == val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "216052c3-3eca-49d4-a694-098694a7caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar(word_vec:np.array, n:int, embeddings:dict, exclude:list):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        word_vec: a word embedding\n",
    "        n: number of similar words to return\n",
    "        embeddings: key word, value embedding\n",
    "        exclude: words to be excluded from the output\n",
    "        \n",
    "    Returns:\n",
    "        n words most similar to word; This does not include words in the exclude list\n",
    "    \"\"\"\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8db413-8978-4c1e-ac99-5aa08f5c9507",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "find_similar(get_word_vector_glove('hello',glove_embeddings), 10, glove_embeddings, exclude=['hello'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d028725-3500-4c31-b16b-484685826f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar(get_word_vector_glove('hello',glove_embeddings), 10, glove_embeddings, exclude=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84a69ee-d299-4ece-8bf1-06a589b56c86",
   "metadata": {},
   "source": [
    "### Part 1.3\n",
    "\n",
    "Answer the following questions: \n",
    "\n",
    "1. What is the time complexity of `find_similar` if your vocab has `v` words, your embedding size is `m`, and you want to find `n` most similar words to the inputted word? \n",
    "\n",
    "2. Consider a scenario (e.g., web application that displays similar words) where you might have to repeatedly run `find_similar`, say for `x` times. What are the benefits and challenges of pre-computing the similarity between all words? How might you overcome the challenges? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7782c9c8-d795-4bae-bf84-3aa3de09509e",
   "metadata": {},
   "source": [
    "## Part 2: Computing similarity between `distilgpt` embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb03d1f-8117-4b8f-8e09-da0eff9bd6a1",
   "metadata": {},
   "source": [
    "### Part 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "361e9379-90da-4c04-b4a2-ef1cf7e066c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/grushaprasad/opt/anaconda3/envs/cosc410/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/grushaprasad/opt/anaconda3/envs/cosc410/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"distilgpt2\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "embedding_matrix = model.get_input_embeddings().weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba6934f-5cca-4d3e-91ae-1108076e2152",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['hello', 'cat', 'chomsky', 'supercalifragilisticexpialidocious']\n",
    "\n",
    "for word in words:\n",
    "    tokens = tokenizer.tokenize(word, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    print(tokens)\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    print(ids)\n",
    "    embeds = embedding_matrix[ids]\n",
    "    print(word, embeds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aa00ac-9096-474c-b5b1-db1ae8363505",
   "metadata": {},
   "source": [
    "#### Answer the following question\n",
    "How does `distilgpt` handle words that are not in its vocab? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082ac9fe-27de-48e7-a906-fd6ffe9d0693",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65b62ca4-a338-4cb6-880c-adb0b284f3ee",
   "metadata": {},
   "source": [
    "### Part 2.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54c0e2b-307a-4fb9-a502-284a000b921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hf_wordvec(word, tokenizer, embedding_matrix):\n",
    "    \"\"\"\n",
    "    Returns an embedding that is the average over all the sub-word tokens\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc74336-c010-449f-8d16-1b9b32bbf8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(get_hf_wordvec('hello', tokenizer, embedding_matrix).shape) =='(768,)')\n",
    "print(str(np.mean(get_hf_wordvec('hello', tokenizer, embedding_matrix))) =='-0.0013018699')\n",
    "print(str(get_hf_wordvec('hello', tokenizer, embedding_matrix)[0]) =='-0.029814368')\n",
    "\n",
    "print(str(get_hf_wordvec('supercalifragilisticexpialidocious', tokenizer, embedding_matrix).shape)=='(768,)')\n",
    "print(str(np.mean(get_hf_wordvec('supercalifragilisticexpialidocious', tokenizer, embedding_matrix)))=='-0.0007369095')\n",
    "print(str(get_hf_wordvec('supercalifragilisticexpialidocious', tokenizer, embedding_matrix)[0]) =='-0.023058223')\n",
    "\n",
    "print(str(get_hf_wordvec('chomsky', tokenizer, embedding_matrix).shape) =='(768,)')\n",
    "print(str(np.mean(get_hf_wordvec('chomsky', tokenizer, embedding_matrix)))=='0.0015589814')\n",
    "print(str(get_hf_wordvec('chomsky', tokenizer, embedding_matrix)[0]) =='0.026135625')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d44943-5bc4-4b13-80c5-dd8e1526f54f",
   "metadata": {},
   "source": [
    "### Part 2.3\n",
    "\n",
    "What are the limitations of using `embedding_matrix` in `find_similar`? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5f1972-daa9-47e9-9e17-fefa76fea61c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2d20495-ba9c-41fe-a099-903930c021b2",
   "metadata": {},
   "source": [
    "### Part 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc192b80-93df-44bc-8a0c-31c601954f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hf_embeddings(hf_model_name: str, vocab:set):\n",
    "    \"\"\"\n",
    "    Returns dictionary. Key: words in the vocab; Value: word embeddings for the hf_model for the word\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790eaddc-7502-4706-9872-54cca532823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add sanity case checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13caf34d-42c1-4786-b8b2-116827dbfcc9",
   "metadata": {},
   "source": [
    "## Part 3: Using analogies to study encoding of gender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cabf86-34f3-4a1d-9242-2207de15727c",
   "metadata": {},
   "source": [
    "### Part 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97add024-89d9-4479-959c-600c20bf506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_analogy(analogy:str, embeddings:dict, n:int):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        analogy: String of the format: a - b + c\n",
    "        embeddings:  key word, value embedding\n",
    "        n: number of cloest words to return\n",
    "    Returns: \n",
    "        n cloest words to the resulting analogy embedding\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "807c32f9-142e-4923-89a5-f164bfda5e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431c2f05-2ae7-4658-9888-7dc61f9582c1",
   "metadata": {},
   "source": [
    "### Part 3.2\n",
    "Come up with at least 10 analogies that you think are important for testing how robustly gender is encoded in some embeddings. Justify why you picked the examples you did. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09ca054-7335-410f-a894-751adae48387",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3772284c-8897-4bb5-8e1b-597264dab598",
   "metadata": {},
   "source": [
    "### Part 3.3\n",
    "Using analogies from the previous part, test how robustly gender is encoded in `GLoVe` and `distilgpt` embeddings. Add as many code and markdown chunks as you would like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4a9d79-dbc0-4c84-b69b-aed3c1e7bb9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26a4869d-be8a-4957-b412-cc56df3872e8",
   "metadata": {},
   "source": [
    "## Part 4 (optional): Using analogies to study encoding of other features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosc410",
   "language": "python",
   "name": "cosc410"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
